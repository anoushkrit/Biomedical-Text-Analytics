{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anoushkrit/Biomedical-Text-Analytics/blob/master/vis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GVpSxAqW27dC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# !{sys.executable} -m spacy download en\n",
        "import re, numpy as np, pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim, spacy, logging, warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import lemmatize, simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwVOlkGO3GD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "4418099d-89d3-4b97-d51a-b01c7e6392a5"
      },
      "cell_type": "code",
      "source": [
        "# Import Dataset\n",
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "df = df.loc[df.target_names.isin(['soc.religion.christian', 'rec.sport.hockey', 'talk.politics.mideast', 'rec.motorcycles']) , :]\n",
        "print(df.shape)  #> (2361, 3)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2361, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>target</th>\n",
              "      <th>target_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
              "      <td>8</td>\n",
              "      <td>rec.motorcycles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10007</th>\n",
              "      <td>From: jet@netcom.Netcom.COM (J. Eric Townsend)...</td>\n",
              "      <td>8</td>\n",
              "      <td>rec.motorcycles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10008</th>\n",
              "      <td>From: gld@cunixb.cc.columbia.edu (Gary L Dare)...</td>\n",
              "      <td>10</td>\n",
              "      <td>rec.sport.hockey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10017</th>\n",
              "      <td>From: nstramer@supergas.dazixco.ingr.com (Naft...</td>\n",
              "      <td>17</td>\n",
              "      <td>talk.politics.mideast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10019</th>\n",
              "      <td>From: mussack@austin.ibm.com (Christopher Muss...</td>\n",
              "      <td>15</td>\n",
              "      <td>soc.religion.christian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content  target  \\\n",
              "10     From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
              "10007  From: jet@netcom.Netcom.COM (J. Eric Townsend)...       8   \n",
              "10008  From: gld@cunixb.cc.columbia.edu (Gary L Dare)...      10   \n",
              "10017  From: nstramer@supergas.dazixco.ingr.com (Naft...      17   \n",
              "10019  From: mussack@austin.ibm.com (Christopher Muss...      15   \n",
              "\n",
              "                 target_names  \n",
              "10            rec.motorcycles  \n",
              "10007         rec.motorcycles  \n",
              "10008        rec.sport.hockey  \n",
              "10017   talk.politics.mideast  \n",
              "10019  soc.religion.christian  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "YtK6qd_Z3IvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a1844a45-4293-43f5-810b-83610e280c94"
      },
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sent in sentences:\n",
        "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
        "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
        "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
        "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
        "        yield(sent)  \n",
        "\n",
        "# Convert to list\n",
        "data = df.content.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1])\n",
        "# [['from', 'irwin', 'arnstein', 'subject', 're', 'recommendation', 'on', 'duc', 'summary', 'whats', 'it', 'worth', 'distribution', 'usa', 'expires', 'sat', 'may', 'gmt', ...trucated...]]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['from', 'irwin', 'arnstein', 'subject', 're', 'recommendation', 'on', 'duc', 'summary', 'whats', 'it', 'worth', 'distribution', 'usa', 'expires', 'sat', 'may', 'gmt', 'organization', 'computrac', 'inc', 'richardson', 'tx', 'keywords', 'ducati', 'gts', 'how', 'much', 'lines', 'have', 'line', 'on', 'ducati', 'gts', 'model', 'with', 'on', 'the', 'clock', 'runs', 'very', 'well', 'paint', 'is', 'the', 'bronze', 'brown', 'orange', 'faded', 'out', 'leaks', 'bit', 'of', 'oil', 'and', 'pops', 'out', 'of', 'st', 'with', 'hard', 'accel', 'the', 'shop', 'will', 'fix', 'trans', 'and', 'oil', 'leak', 'they', 'sold', 'the', 'bike', 'to', 'the', 'and', 'only', 'owner', 'they', 'want', 'and', 'am', 'thinking', 'more', 'like', 'any', 'opinions', 'out', 'there', 'please', 'email', 'me', 'thanks', 'it', 'would', 'be', 'nice', 'stable', 'mate', 'to', 'the', 'beemer', 'then', 'ill', 'get', 'jap', 'bike', 'and', 'call', 'myself', 'axis', 'motors', 'tuba', 'irwin', 'honk', 'therefore', 'am', 'computrac', 'richardson', 'tx', 'dod']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uKN3HOcC3RrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "61cb39fc-fba7-4e7e-e746-7102571a45a3"
      },
      "cell_type": "code",
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# !python3 -m spacy download en  # run in terminal once\n",
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
        "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "    texts = [bigram_mod[doc] for doc in texts]\n",
        "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "    texts_out = []\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    # remove stopwords once more after lemmatization\n",
        "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
        "    return texts_out\n",
        "\n",
        "data_ready = process_words(data_words)  # processed Text Data!"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "GY6CXAmi3XHq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "\n",
        "# Create Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=4, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=10,\n",
        "                                           passes=10,\n",
        "                                           alpha='symmetric',\n",
        "                                           iterations=100,\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())\n",
        "#> [(0,\n",
        "#>   '0.017*\"write\" + 0.015*\"people\" + 0.014*\"organization\" + 0.014*\"article\" + '\n",
        "#>   '0.013*\"time\" + 0.008*\"give\" + 0.008*\"first\" + 0.007*\"tell\" + 0.007*\"new\" + '\n",
        "#>   '0.007*\"question\"'),\n",
        "#>  (1,\n",
        "#>   '0.008*\"christian\" + 0.008*\"believe\" + 0.007*\"god\" + 0.007*\"law\" + '\n",
        "#>   '0.006*\"state\" + 0.006*\"israel\" + 0.006*\"israeli\" + 0.005*\"exist\" + '\n",
        "#>   '0.005*\"way\" + 0.004*\"bible\"'),\n",
        "#>  (2,\n",
        "#>   '0.024*\"armenian\" + 0.012*\"bike\" + 0.006*\"kill\" + 0.006*\"work\" + '\n",
        "#>   '0.005*\"well\" + 0.005*\"year\" + 0.005*\"sumgait\" + 0.005*\"soldier\" + '\n",
        "#>   '0.004*\"way\" + 0.004*\"ride\"'),\n",
        "#>  (3,\n",
        "#>   '0.019*\"team\" + 0.019*\"game\" + 0.013*\"hockey\" + 0.010*\"player\" + '\n",
        "#>   '0.009*\"play\" + 0.009*\"win\" + 0.009*\"nhl\" + 0.009*\"year\" + 0.009*\"hawk\" + '\n",
        "#>   '0.009*\"season\"')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O3rwLoZS3aoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "df_dominant_topic.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "alM--htv3eI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Display setting to show more characters in column\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
        "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd:\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index    \n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zrMYu1GR3h15",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc_lens = [len(d) for d in df_dominant_topic.Text]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(16,7), dpi=160)\n",
        "plt.hist(doc_lens, bins = 1000, color='navy')\n",
        "plt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
        "plt.text(750,  90, \"Median : \" + str(round(np.median(doc_lens))))\n",
        "plt.text(750,  80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
        "plt.text(750,  70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
        "plt.text(750,  60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
        "\n",
        "plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\n",
        "plt.tick_params(size=16)\n",
        "plt.xticks(np.linspace(0,1000,9))\n",
        "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4y10okzH3k8I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.colors as mcolors\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "fig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):    \n",
        "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
        "    doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n",
        "    ax.hist(doc_lens, bins = 1000, color=cols[i])\n",
        "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
        "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
        "    ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n",
        "    ax.set_ylabel('Number of Documents', color=cols[i])\n",
        "    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=0.90)\n",
        "plt.xticks(np.linspace(0,1000,9))\n",
        "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kbMSZqI63qM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1. Wordcloud of Top N words in each topic\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "cloud = WordCloud(stopwords=stop_words,\n",
        "                  background_color='white',\n",
        "                  width=2500,\n",
        "                  height=1800,\n",
        "                  max_words=10,\n",
        "                  colormap='tab10',\n",
        "                  color_func=lambda *args, **kwargs: cols[i],\n",
        "                  prefer_horizontal=1.0)\n",
        "\n",
        "topics = lda_model.show_topics(formatted=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    fig.add_subplot(ax)\n",
        "    topic_words = dict(topics[i][1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "    plt.gca().imshow(cloud)\n",
        "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "    plt.gca().axis('off')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x6E8_0xs3s9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "topics = lda_model.show_topics(formatted=False)\n",
        "data_flat = [w for w_list in data_ready for w in w_list]\n",
        "counter = Counter(data_flat)\n",
        "\n",
        "out = []\n",
        "for i, topic in topics:\n",
        "    for word, weight in topic:\n",
        "        out.append([word, i , weight, counter[word]])\n",
        "\n",
        "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
        "\n",
        "# Plot Word Count and Weights of Topic Keywords\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
        "    ax_twin = ax.twinx()\n",
        "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
        "    ax.set_ylabel('Word Count', color=cols[i])\n",
        "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
        "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
        "    ax.tick_params(axis='y', left=False)\n",
        "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
        "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
        "\n",
        "fig.tight_layout(w_pad=2)    \n",
        "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EBb9QDEe3wkc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Sentence Coloring of N Sentences\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "def sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n",
        "    corp = corpus[start:end]\n",
        "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
        "\n",
        "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
        "    axes[0].axis('off')\n",
        "    for i, ax in enumerate(axes):\n",
        "        if i > 0:\n",
        "            corp_cur = corp[i-1] \n",
        "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
        "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
        "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n",
        "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
        "\n",
        "            # Draw Rectange\n",
        "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
        "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
        "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
        "\n",
        "            word_pos = 0.06\n",
        "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
        "                if j < 14:\n",
        "                    ax.text(word_pos, 0.5, word,\n",
        "                            horizontalalignment='left',\n",
        "                            verticalalignment='center',\n",
        "                            fontsize=16, color=mycolors[topics],\n",
        "                            transform=ax.transAxes, fontweight=700)\n",
        "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
        "                    ax.axis('off')\n",
        "            ax.text(word_pos, 0.5, '. . .',\n",
        "                    horizontalalignment='left',\n",
        "                    verticalalignment='center',\n",
        "                    fontsize=16, color='black',\n",
        "                    transform=ax.transAxes)       \n",
        "\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.95, fontweight=700)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "sentences_chart()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mGpYsj_w3zsf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Sentence Coloring of N Sentences\n",
        "def topics_per_document(model, corpus, start=0, end=1):\n",
        "    corpus_sel = corpus[start:end]\n",
        "    dominant_topics = []\n",
        "    topic_percentages = []\n",
        "    for i, corp in enumerate(corpus_sel):\n",
        "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
        "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
        "        dominant_topics.append((i, dominant_topic))\n",
        "        topic_percentages.append(topic_percs)\n",
        "    return(dominant_topics, topic_percentages)\n",
        "\n",
        "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
        "\n",
        "# Distribution of Dominant Topics in Each Document\n",
        "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
        "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
        "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
        "\n",
        "# Total Topic Distribution by actual weight\n",
        "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
        "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
        "\n",
        "# Top 3 Keywords for each Topic\n",
        "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
        "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
        "\n",
        "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
        "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
        "df_top3words.reset_index(level=0,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lm6De-gB31lc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n",
        "\n",
        "# Topic Distribution by Dominant Topics\n",
        "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
        "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
        "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
        "ax1.xaxis.set_major_formatter(tick_formatter)\n",
        "ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
        "ax1.set_ylabel('Number of Documents')\n",
        "ax1.set_ylim(0, 1000)\n",
        "\n",
        "# Topic Distribution by Topic Weights\n",
        "ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
        "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
        "ax2.xaxis.set_major_formatter(tick_formatter)\n",
        "ax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xBqx7hGA34pz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get topic weights and dominant topics ------------\n",
        "from sklearn.manifold import TSNE\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models import Label\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "# Get topic weights\n",
        "topic_weights = []\n",
        "for i, row_list in enumerate(lda_model[corpus]):\n",
        "    topic_weights.append([w for i, w in row_list[0]])\n",
        "\n",
        "# Array of topic weights    \n",
        "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
        "\n",
        "# Keep the well separated points (optional)\n",
        "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
        "\n",
        "# Dominant topic number in each doc\n",
        "topic_num = np.argmax(arr, axis=1)\n",
        "\n",
        "# tSNE Dimension Reduction\n",
        "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
        "tsne_lda = tsne_model.fit_transform(arr)\n",
        "\n",
        "# Plot the Topic Clusters using Bokeh\n",
        "output_notebook()\n",
        "n_topics = 4\n",
        "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
        "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
        "              plot_width=900, plot_height=700)\n",
        "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
        "show(plot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PWfi-x137OU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
        "vis"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}